{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email\n",
    "\n",
    "## Mylu Praveen\n",
    "\n",
    "\n",
    "\n",
    "1) Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "\n",
    "In this Project our goal is to identify the person committing fraud in Enron or to find out the person of intrest(used by US law for identifying person committed crimes).\n",
    "Machine learning techniques helps us to identify the person involving in fraud.\n",
    "\n",
    "\n",
    "Data Exploration:\n",
    "\n",
    "a) Total number of data points = 146\n",
    "\n",
    "b) Number of features = 21\n",
    "\n",
    "Size of each feature = 146\n",
    "\n",
    "Total Count of values = Count of non missing values + Count of missing values\n",
    "\n",
    "   b1) bonus: 82 + 64\n",
    "   \n",
    "   b2) deferral_payments: 39 + 107\n",
    "   \n",
    "   b3) deferred_income: 49 + 97\n",
    "   \n",
    "   b4) director_fees: 17 + 129\n",
    "   \n",
    "   b5) email_address: 111 + 35\n",
    "   \n",
    "   b6) exercised_stock_options: 102 + 44\n",
    "   \n",
    "   b7) expenses: 95 + 51\n",
    "   \n",
    "   b8) from_messages: 86 + 60\n",
    "   \n",
    "   b9) from_poi_to_this_person: 86 + 60\n",
    "   \n",
    "   b10) from_this_person_to_poi: 86 + 60\n",
    "   \n",
    "   b11) loan_advances: 4 +  142\n",
    "   \n",
    "   b12) long_term_incentive: 66 + 80\n",
    "   \n",
    "   b13) other: 93 + 53\n",
    "   \n",
    "   b14) poi: 146 + 0\n",
    "   \n",
    "   b15) restricted_stock: 110 + 36\n",
    "   \n",
    "   b16) restricted_stock_deferred: 18 + 128\n",
    "   \n",
    "   b17) salary:  95 + 51\n",
    "   \n",
    "   b18) shared_receipt_with_poi: 86 + 60\n",
    "   \n",
    "   b19) to_messages: 86 + 60\n",
    "   \n",
    "   b20) total_payments: 125 + 21\n",
    "   \n",
    "   b21) total_stock_value: 126 + 20\n",
    " \n",
    "We can observe  that there are no missing values for POI\n",
    "   \n",
    "c) Non-POI = 128\n",
    "\n",
    "d) POI = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVLBUJAkFQwFSVLBUJAkFQwFSVLh/wP1B+q3c2iFtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb69db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "def Outliers(my_dataset, feature_x, feature_y):\n",
    "    \n",
    "    data = featureFormat(my_dataset, [feature_x, feature_y])\n",
    "    for point in data:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        matplotlib.pyplot.scatter( x, y )\n",
    "    matplotlib.pyplot.xlabel(feature_x)\n",
    "    matplotlib.pyplot.ylabel(feature_y)\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "print(Outliers(data_dict, 'salary', 'bonus'))\n",
    "identity = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier Investigation:\n",
    "\n",
    "By this scatter plot for salary vs bonus we can identify an outlier named \"TOTAL\" which will be removed. \"THE TRAVEL AGENCY IN THE PARK\" also need to be removed because it is an travel agency. We remove \"LOCKHART EUGENE E\" which dont have any value other than NaN.\n",
    "\n",
    "\n",
    "2) What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset — explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importance of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.\n",
    "\n",
    "I have created two new features as (fraction_from_poi) and (fraction_to_poi).\n",
    "fraction_from_poi states that the number of emails received from POIs and fraction_to_poi states that the number of emails sent to POIs. By creating this features we may get to know the person sending and receiving email with POIs might be POI himself. We can think of a logic that mostly POIs will be much in contact with each other than with Non-POIs, where it would be helpful to improve classifier's performance.\n",
    "\n",
    "An automated feature selection function SelectKBest with k =7 is used for identifying features.\n",
    "There are totally 23 features in the dataset in which no all features need to be used. I have choosen k = 7 to obtain 7 highest scoring features where remaining features were to be removed based on the k scores using SelectKBest univariate statistical tests. We randomly divide the Dataset into a training dataset with 70% and a testing dataset with 30%.\n",
    "\n",
    "I tried testing the classifier with some values of k and got accuracy, precision and recall as:\n",
    "\n",
    "k = 7 : Accuracy = 0.84671 , Precision = 0.45660 , Recall = 0.38400  \n",
    "\n",
    "k = 8 : Accuracy = 0.84100, Precision = 0.38355 , Recall = 0.31700  \n",
    "\n",
    "k = 9 : Accuracy = 0.83613, Precision = 0.36639, Recall = 0.31400  \n",
    "\n",
    "k = 15 : Accuracy = 0.81353, Precision = 0.30437, Recall = 0.31000 \n",
    "\n",
    "k = 20 : Accuracy = 0.77020, Precision = 0.21392, Recall = 0.27050 \n",
    "\n",
    "k = 22 : Accuracy = 0.77320, Precision = 0.21870, Recall = 0.27250 \n",
    "\n",
    "We get better accuracy, precision and recall for k = 7 as much better compared to remaining values for obtaining k scores. so i choosed the value as k = 7.\n",
    "\n",
    "total number of features are 28\n",
    "\n",
    "K Scores for the features are\n",
    "\n",
    "a) exercised_stock_options = 24.815079733218194)\n",
    "\n",
    "b) total_stock_value = 24.182898678566879)\n",
    "\n",
    "c) bonus = 20.792252047181535)\n",
    "\n",
    "d) salary = 18.289684043404513)\n",
    "\n",
    "e) deferred_income = 11.458476579280369)\n",
    "\n",
    "f) long_term_incentive = 9.9221860131898225)\n",
    "\n",
    "g) restricted_stock = 9.2128106219771002)\n",
    "\n",
    "h) total_payments = 8.7727777300916756)\n",
    "\n",
    "i) shared_receipt_with_poi = 8.589420731682381)\n",
    "\n",
    "j) loan_advances = 7.1840556582887247)\n",
    "\n",
    "k) expenses = 6.0941733106389453)\n",
    "\n",
    "l) from_poi_to_this_person = 5.2434497133749582)\n",
    "\n",
    "m) fraction_from_poi = 5.1239461527568926)\n",
    "\n",
    "n) other = 4.1874775069953749)\n",
    "\n",
    "o) fraction_to_poi = 4.0946533095769526)\n",
    "\n",
    "p) from_this_person_to_poi = 2.3826121082276739)\n",
    "\n",
    "q) director_fees = 2.1263278020077054)\n",
    "\n",
    "r) to_messages = 1.6463411294420076)\n",
    "\n",
    "s) bonus_salary_ratio = 0.62853011823446625)\n",
    "\n",
    "t) deferral_payments = 0.22461127473600989)\n",
    "\n",
    "u) from_messages = 0.16970094762175533)\n",
    "\n",
    "v) total_net_worth = 0.085619908360810876)\n",
    "\n",
    "w) restricted_stock_deferred = 0.065499652909942141)\n",
    "\n",
    "features list = ['poi', 'salary', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock']\n",
    "\n",
    "exercised_stock_options and total_stock_value are the most highest K scores compared to all the features.\n",
    "\n",
    "All these features were rescaled using the sklearn min/max scaler where as the email data points and feature data points are measured by different scales. k-nearest neighbors with an Euclidean distance measure, where features will contribute equally.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3) What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms? [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "\n",
    "I have used mulitple algorithms such as:\n",
    "\n",
    "a) Gaussian NB:\n",
    "\n",
    "   Accuracy = 0.84671\n",
    "\n",
    "   Precision = 0.45660\n",
    "\n",
    "   Recall = 0.38400 \n",
    "\n",
    "   F1 = 0.41716     \n",
    "\n",
    "   F2 = 0.39661\n",
    "\n",
    "   Total predictions = 14000        \n",
    "\n",
    "   True positives = 768    \n",
    "\n",
    "   False positives = 914   \n",
    "\n",
    "   False negatives = 1232   \n",
    "\n",
    "   True negatives = 11086\n",
    "\n",
    "\n",
    "b) Linear SVC:\n",
    "\n",
    "   Accuracy = 0.66193\n",
    "   \n",
    "   Precision = 0.13374\n",
    "   \n",
    "   Recall = 0.24950\n",
    "   \n",
    "   F1 = 0.17414\n",
    "   \n",
    "   F2 = 0.21268\n",
    "   \n",
    "   Total predictions = 14000\n",
    "   \n",
    "   True positives = 499\n",
    "   \n",
    "   False positives = 3232\n",
    "   \n",
    "   False negatives = 1501\n",
    "   \n",
    "   True negatives = 8768\n",
    "\n",
    "\n",
    "c) DecissionTreeclassifier:\n",
    "   \n",
    "   Accuracy = 0.78650\n",
    "   \n",
    "   Precision = 0.24758\n",
    "   \n",
    "   Recall = 0.24250\n",
    "   \n",
    "   F1 = 0.24501\n",
    "   \n",
    "   F2 = 0.24350\n",
    "   \n",
    "   Total predictions = 14000\n",
    "   \n",
    "   True positives =  485\n",
    "   \n",
    "   False positives = 1474\n",
    "   \n",
    "   False negatives = 1515\n",
    "   \n",
    "   True negatives = 10526\n",
    "\n",
    "\n",
    "d) KNeighborsClassifier:\n",
    "\n",
    "   Accuracy = 0.87650\n",
    "   \n",
    "   Precision = 0.68385\n",
    "   \n",
    "   Recall = 0.25200\n",
    "   \n",
    "   F1 = 0.36829\n",
    "   \n",
    "   F2 = 0.28843\n",
    "   \n",
    "   Total predictions = 14000\n",
    "   \n",
    "   True positives =  504\n",
    "   \n",
    "   False positives =  233\n",
    "   \n",
    "   False negatives = 1496\n",
    "   \n",
    "   True negatives = 11767\n",
    "   \n",
    "   \n",
    "Compared to all algorithms or classifiers Gaussian NB has a good accuracy, precision and recall scores where KNeighborsClassifier also have an good accuracy and precision but low recall than Gaussain NB. DecisionTreeClassifier and Linear SVC tends to have low accuracy, precision and recall scores compared to Gaussian NB and KNeighborsClassifier.\n",
    "\n",
    "To standardize the range we use a method called feature scaling. It was performed for both KNeighbors and Linear SVC.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune — if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).\n",
    "\n",
    "\n",
    "Tuning parameter is used in statistics algorithm in order to control their behaviour. The algorithm creates normally for each value of the tuning parameter a different model.  It also helps to improve performanced. \n",
    "\n",
    "If we are not performing this tuning parameter then we may not able to get better perfomance for the algorithm.\n",
    "\n",
    "I have used multiple algorithms and used GridSearchCV function to get the best parameters and F1 is used as the scoring method. Algorithms used are:\n",
    "\n",
    "a) GaussianNB\n",
    "\n",
    "b) Linear SVC\n",
    "\n",
    "c) DecisionTreeClassifier\n",
    "\n",
    "d) KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "Accuracy =  0.93023255814\n",
    "\n",
    "Precision = 0.4\n",
    "\n",
    "Recall = 1.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5) What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "\n",
    "\n",
    "In machine learning, validation is the process where a trained model is evaluated with a testing data set. It is the process of testing our algorithm performance by splitting the dataset into separate training and test data. \n",
    "Our idea is to test the model in the training phase.\n",
    "\n",
    "we separate data in three parts as training, cross-validation and test sets. so model is tuned to maximize the evaluation score on the cross-validation set, and then the finally model efficiency will be measured on the test set.\n",
    "\n",
    "Since there are too few observations for us to train and test the algorithms, in order to extract the most information from the data, the selected strategy to validate our model was a Nested Stratified Shuffle Split Cross-Validation.\n",
    "\n",
    "The tester.py script is used to measure the results for each of the iteration. Stratified ShuffleSplit cross-validator is used which provides training and testing indices to split data in train and test sets.\n",
    "\n",
    "The tester.py script uses number of iterations of the StratifiedShuffleSplit. Iteration StratifiedShuffleSplit will be added to the poi_id.py script in order to validate faster.\n",
    "\n",
    "we prefer smaller data for training part, where using whole data we cant train it. Train test split will be used to validate the performance. We split the data into training data and test data and calculate the accuracy, precision, and recall for each of the iteration.\n",
    "A single split into a training & testing set wont give a better estimate of error accuracy, so the data need to be randomly splitted into k folds. BY this validator we can independently choose how large each test set is and how many trials you average over. \n",
    "\n",
    "Validation in machine learning consists of evaluating a model using data that was not touched during the training process. A classic mistake is to ignore this rule, hence obtaining overly optimistic results due to overfitting the training data, but very poor performance on unseen data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6) Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "\n",
    "\n",
    "Precision and Recall are the two evaluation metrics used and Naive bayes is the best algorithm that can match in this project. \n",
    "\n",
    "KNeighborsClassifier have an good accuracy and precision but it has a low recall score compared to Gaussain NB. so we prefer Gaussian NB as the best algorithm as it has an  Accuracy: 0.84671, Precision: 0.45660 and Recall: 0.38400.\n",
    "\n",
    "DecisionTreeClassifier and Linear SVC tends to have low precision and recall scores compared to Gaussian NB and KNeighborsClassifier. where as KneighborClassifier has true positives of 504 and false positives of 233.\n",
    "\n",
    "Precision is the ratio of right classifications over all observations with a given predicted label. For example, the ratio of true POI’s over all predicted POI’s.\n",
    "\n",
    "Recall is the ratio of right classifications over all observations that are truly of a given class. For example, the ratio of observations correctly labeled POI over all true POI’s.\n",
    "\n",
    "Finally for the selected model GuassianNB we get our scores as:\n",
    "\n",
    "Accuracy = 0.84671       \n",
    "\n",
    "Precision = 0.45660\n",
    "\n",
    "Recall = 0.38400\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
